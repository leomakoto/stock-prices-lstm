{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT'S THIS?**\n",
    "\n",
    "This is a very, very simple and minimalist model for predicting Brazilian stock close prices over almost 30-year worth of data, using a basic version of a Long Short-Term Memory (LSTM) Neural Network.\n",
    "\n",
    "My dataset was download from here: https://www.kaggle.com/felsal/ibovespa-stocks\n",
    "\n",
    "The idea of the problem is pretty simple: you feed the model with observations from the past X days and it tells you what it thinks will be the close price of a given stock Y days ahead.\n",
    "\n",
    "**INPUT**\n",
    "\n",
    "The input dictionary for this code is mostly self-explanatory: 'features' is what you're going to consider in your prediction, 'target' is what you want to predict, 'size' is the X from above, 'ahead' is Y, 'tick' is your favorite stock identifier, 'ratio' is training ratio, and the remaining entries are more or less jargon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = {\n",
    "\t'features' : ['close'],\n",
    "\t'target'   : ['close'],\n",
    "\t'size'     : 10,\n",
    "\t'ahead'    : 1,\n",
    "\t'tick'     : \"ITUB4\",   #Ita√∫!\n",
    "\t'ratio'    : 0.20,      #Test size\n",
    "\t'layers'   : 3,\n",
    "\t'neurons'  : [150, 50, 50],  #For each layer\n",
    "\t'epochs'   : 100,\n",
    "\t'batch'    : 128  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THE MAIN FUNCTIONS**\n",
    "\n",
    "There are only two important functions in this code, this the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a DF vector, normalizes it, and splits it into a number of size-subvectors. Returns a list of such size-subvectors plus a vector of ahead-vectors with future values.\n",
    "def windowsize(data, size, ahead):\n",
    "\tlx, ly = [], []\n",
    "\tfor i in range(len(data)-size-ahead):\n",
    "\t\tx=data[i:(i+size), :]\n",
    "\t\ty=data[i+size+ahead-1, 0].squeeze()\n",
    "\t\tlx.append(x)\n",
    "\t\tly.append(y)\n",
    "\treturn np.array(lx), np.array(ly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It simply divides your whole dataset into a bunch of tiny datasets as it is common practice in recurrent neural networks.\n",
    "\n",
    "The other important function is, of course, the prediction function which takes the alraedy divided data, normalizes it, and feeds it to the pre-coded LSTM model from KERAS. Then, it plots the testing results and returns the prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM predictor\n",
    "def lstmpred(data, inpt):\n",
    "\texdata = data[data.ticker == inpt['tick']]\n",
    "\tnpdata = np.array(exdata[inpt['features']])\n",
    "\t\n",
    "\t#Labels (dates)\n",
    "\tlabels = list(map(str, exdata['datetime'].values))\n",
    "\t\n",
    "\t#Scale data\n",
    "\tscaler=mms(feature_range=(-1,1))\n",
    "\tnpdata=scaler.fit_transform(npdata)\n",
    "\n",
    "\t#Split data in smaller windows\n",
    "\tnx, ny = windowsize(npdata, inpt['size'], inpt['ahead'])\n",
    "\n",
    "\t#Splitting into train and test sets\n",
    "\tntrnx, ntstx, ntrny, ntsty = tts([nx, ny], inpt['ratio'])\n",
    "\t\n",
    "\t#Build model\n",
    "\tmodel = Sequential()\n",
    "\tfor i in range(inpt['layers']):\n",
    "\t\tmodel.add(LSTM(inpt['neurons'][i], dropout=0.2, return_sequences = True, input_shape = (ntrnx.shape[1], len(inpt['features']))))\n",
    "\tmodel.add(LSTM(1, dropout=0.2, return_sequences= False))\n",
    "\n",
    "\t#Compile and fit model\n",
    "\tmodel.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\tmodel.fit(ntrnx, ntrny, validation_data=(ntstx, ntsty), batch_size=inpt['batch'], epochs=inpt['epochs'])\n",
    "\n",
    "\t#Test\n",
    "\tnpreds = model.predict(ntstx)\n",
    "\tpreds = scaler.inverse_transform(npreds)\n",
    "\ttsty = scaler.inverse_transform(ntsty.reshape(-1,1))\n",
    "\n",
    "\tplt.figure(figsize=(20,10))\n",
    "\txaxis = range(len(preds))\n",
    "\tplt.plot(xaxis,preds.squeeze(), color=\"red\")\n",
    "\tplt.plot(xaxis,tsty.squeeze(), color=\"blue\")\n",
    "\tplt.show()\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST RESULTS**\n",
    "\n",
    "This is how our model performed trying to predict the close prices of ITUB4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
